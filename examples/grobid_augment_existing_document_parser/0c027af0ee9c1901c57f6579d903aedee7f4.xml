<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Deep Models for Overhead Image Segmentation Through Getis-Ord Gi* Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-23">23 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,127.12,164.94,70.06,10.37"><forename type="first">Xueqing</forename><surname>Deng</surname></persName>
							<email>xdeng7@ucmerced.edu</email>
						</author>
						<author>
							<persName coords="1,134.09,178.89,56.11,10.37"><forename type="first">U</forename><forename type="middle">C</forename><surname>Merced</surname></persName>
						</author>
						<author>
							<persName coords="1,280.84,164.94,33.55,10.37"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName coords="1,259.17,212.76,76.88,10.37"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
						</author>
						<title level="a" type="main">Generalizing Deep Models for Overhead Image Segmentation Through Getis-Ord Gi* Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-23">23 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF1FB7B296C20D88FD636D7849E5F6D6</idno>
					<idno type="arXiv">arXiv:1912.10667v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-05-12T20:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s coords="1,62.07,308.34,224.30,8.59;1,50.11,320.30,141.76,8.59">That most deep learning models are purely data driven is both a strength and a weakness.</s><s coords="1,197.54,320.30,88.83,8.59;1,50.11,332.25,236.25,8.59;1,50.11,344.21,46.05,8.59">Given sufficient training data, the optimal model for a particular problem can be learned.</s><s coords="1,102.91,344.21,183.45,8.59;1,50.11,356.17,236.25,8.59;1,50.11,368.12,236.25,8.59;1,50.11,380.08,117.61,8.59">However, this is usually not the case and so instead the model is either learned from scratch from a limited amount of training data or pre-trained on a different problem and then fine-tuned.</s><s coords="1,173.76,380.08,112.61,8.59;1,50.11,392.03,236.25,8.59;1,50.11,403.99,26.84,8.59">Both of these situations are potentially suboptimal and limit the generalizability of the model.</s><s coords="1,82.50,403.99,203.86,8.59;1,50.11,415.94,236.25,8.59;1,50.11,427.90,236.25,8.59;1,50.11,439.85,236.25,8.59;1,50.11,451.81,180.75,8.59">Inspired by this, we investigate methods to inform or guide deep learning models for geospatial image analysis to increase their performance when a limited amount of training data is available or when they are applied to scenarios other than which they were trained on.</s><s coords="1,233.91,451.81,52.45,8.59;1,50.11,463.76,236.25,8.59;1,50.11,475.72,236.25,8.59;1,50.11,487.67,236.25,8.59">In particular, we exploit the fact that there are certain fundamental rules as to how things are distributed on the surface of the Earth and these rules do not vary substantially between locations.</s><s coords="1,50.11,499.63,236.25,8.59;1,50.11,511.44,208.74,8.74;1,258.85,509.87,4.08,6.12;1,258.85,516.08,2.82,6.12;1,265.33,511.58,21.03,8.59;1,50.11,523.54,90.47,8.59">Based on this, we develop a novel feature pooling method for convolutional neural networks using Getis-Ord G * i analysis from geostatistics.</s><s coords="1,144.45,523.54,141.92,8.59;1,50.11,535.49,236.25,8.59;1,50.11,547.45,236.25,8.59;1,50.11,559.40,217.58,8.59">Experimental results show our proposed pooling function has significantly better generalization performance compared to a standard data-driven approach when applied to overhead image segmentation.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." coords="1,50.11,598.52,76.84,10.75">Introduction</head><p><s coords="1,62.07,620.83,224.30,8.64;1,50.11,632.78,209.83,8.64">Research in remote sensing has been steadily increasing since it is an important source for Earth observation.</s><s coords="1,263.48,632.78,22.88,8.64;1,50.11,644.74,236.25,8.64;1,50.11,656.69,236.25,8.64;1,50.11,668.65,120.55,8.64">Overhead imagery can easily be acquired using low-cost drones and no longer requires access to expensive high-resolution satellite or airborne platforms.</s><s coords="1,173.74,668.65,112.63,8.64;1,50.11,680.60,236.25,8.64;1,50.11,692.56,236.25,8.64;1,50.11,704.51,236.25,8.64;1,308.86,401.32,138.54,8.64">Since the data provides convenient and large-scale coverage, people are using it for a number of societally important problems such as traffic monitoring <ref type="bibr" coords="1,97.49,704.51,15.27,8.64" target="#b19">[21]</ref>, urban planning <ref type="bibr" coords="1,183.57,704.51,10.58,8.64" target="#b3">[4]</ref>, vehicle detection <ref type="bibr" coords="1,272.26,704.51,10.58,8.64" target="#b8">[9]</ref>, Figure <ref type="figure" coords="1,337.94,401.32,3.88,8.64">1</ref>: Motivation of our work.</s><s coords="1,452.23,401.32,92.88,8.64;1,308.86,413.28,198.31,8.64">The content in the current sliding window is a cluster of pixels of tree.</s><s coords="1,512.31,413.28,32.81,8.64;1,308.86,425.23,236.25,8.64;1,308.86,437.19,236.25,8.64;1,308.86,449.14,236.25,8.64">We propose to incorporate geospatial knowledge to build a pooling function which can propagate such a spatial cluster during training, while the standard pooling is not able to achieve it.</s><s coords="1,308.86,485.97,234.39,8.64">land cover segmentation <ref type="bibr" coords="1,408.30,485.97,15.27,8.64" target="#b15">[17]</ref>, building extraction <ref type="bibr" coords="1,507.57,485.97,15.27,8.64" target="#b34">[36]</ref>, etc.</s></p><p><s coords="1,320.82,499.60,224.30,8.64;1,308.86,511.56,236.25,8.64;1,308.86,523.51,236.25,8.64;1,308.86,535.47,126.69,8.64">Recently, the analysis of overhead imagery has benefited greatly from deep learning thanks to the significant advancements made by the computer vision community on regular (non-overhead) images.</s><s coords="1,441.37,535.47,103.74,8.64;1,308.86,547.42,236.25,8.64;1,308.86,559.38,236.25,8.64;1,308.86,571.33,236.25,8.64;1,308.86,583.29,175.69,8.64">However, there still often remains challenges when adapting these deep learning techniques to overhead image analysis, such as the limited availability of labeled overhead imagery, the difficulty of the models to generalize between locations, etc.</s></p><p><s coords="1,320.82,596.92,224.30,8.64;1,308.86,608.87,236.25,8.64;1,308.86,620.83,236.25,8.64">Annotating overhead imagery is labor intensive so existing datasets are often not large enough to train effective convolutional neural networks (CNNs) from scratch.</s><s coords="1,308.86,632.78,236.25,8.64;1,308.86,644.74,236.25,8.64;1,308.86,656.69,57.39,8.64">A common practice therefore is to fine-tune an ImageNet pre-trained model on a small amount of annotated overhead imagery.</s><s coords="1,374.17,656.69,170.95,8.64;1,308.86,668.65,236.25,8.64;1,308.86,680.60,147.20,8.64">However, the generalization capability of fine-tuned models is limited as models trained on one location may not work well on others.</s><s coords="1,460.99,680.60,84.12,8.64;1,308.86,692.38,236.25,8.82;1,308.86,704.51,236.25,8.64;2,50.11,75.48,236.25,8.64;2,50.11,87.43,101.97,8.64">This is known as the cross-location generalization problem and is not necessarily limited to overhead image analysis as it can also be a challenge for ground-level imagery such as cross-city road scene segmentation <ref type="bibr" coords="2,132.99,87.43,15.27,8.64" target="#b9">[10]</ref>.</s><s coords="2,159.35,87.43,127.01,8.64;2,50.11,99.39,236.25,8.64;2,50.11,111.34,236.25,8.64;2,50.11,123.30,236.25,8.64;2,50.11,135.25,123.48,8.64">Deep models are often overfitting due to their large capacity yet generalization is particularly important for overhead images since they can look quite different due to variations in the seasons, position of the sun, location variation, etc.</s><s coords="2,177.43,135.25,108.93,8.64;2,50.11,147.21,236.25,8.64;2,50.11,159.16,236.25,8.64;2,50.11,171.12,46.81,8.64">For regular image analysis, two widely adopted approaches to overcome these so-called domain gaps include domain adaptation <ref type="bibr" coords="2,210.88,159.16,15.77,8.64">[12,</ref><ref type="bibr" coords="2,227.79,159.16,12.45,8.64" target="#b11">13,</ref><ref type="bibr" coords="2,241.36,159.16,9.41,8.64" target="#b30">[32]</ref><ref type="bibr" coords="2,250.77,159.16,4.70,8.64" target="#b31">[33]</ref><ref type="bibr" coords="2,255.48,159.16,14.11,8.64" target="#b32">[34]</ref> and data fusion.</s><s coords="2,101.00,171.12,185.36,8.64;2,50.11,183.07,236.25,8.64;2,50.11,195.03,44.35,8.64">Both approaches have been adapted by the remote sensing community <ref type="bibr" coords="2,155.62,183.07,11.62,8.64" target="#b1">[2]</ref> to improve performance and robustness.</s></p><p><s coords="2,62.07,207.04,224.30,8.64;2,50.11,219.00,129.14,8.64">In this paper, we take a different, novel approach to address the domain gap problem.</s><s coords="2,187.29,219.00,99.08,8.64;2,50.11,230.95,236.25,8.64;2,50.11,242.91,236.25,8.64;2,50.11,254.86,38.47,8.64">We exploit the fact that things are not laid out at random on the surface of the Earth and that this structure does not vary substantially between locations.</s><s coords="2,93.22,254.86,193.14,8.64;2,50.11,266.82,236.25,8.64;2,50.11,278.77,236.25,8.64;2,50.11,290.73,107.73,8.64">In particular, we pose the question of how prior knowledge of this structure or, more interestingly, how the fundamental rules of geography might be incorporated into general CNN frameworks.</s><s coords="2,164.94,290.73,121.42,8.64;2,50.11,302.68,236.25,8.64;2,50.11,314.64,236.25,8.64;2,50.11,326.59,15.22,8.64">Inspired by work on physicsguided neural networks <ref type="bibr" coords="2,149.85,302.68,15.27,8.64" target="#b13">[15]</ref>, we develop a framework in which spatial hotspot analysis informs the feature map pooling.</s><s coords="2,70.92,326.41,215.44,8.82;2,50.11,338.23,26.16,8.74;2,76.28,336.65,4.08,6.12;2,76.28,342.86,2.82,6.12;2,83.50,338.37,202.86,8.82;2,50.11,350.50,236.25,8.64;2,50.11,362.46,97.84,8.64">We term this geo-constrained pooling strategy Getis-Ord G * i pooling and show that it significantly improves the semantic segmentation of overhead imagery particularly in cross-location scenarios.</s><s coords="2,151.01,362.46,135.35,8.64;2,50.11,374.41,236.25,8.64;2,50.11,386.37,140.94,8.64">To our knowledge, ours is the first work to incorporate geo-spatial knowledge directly into the fundamental mechanisms of CNNs.</s><s coords="2,194.00,386.37,92.36,8.64;2,50.11,398.32,128.78,8.64">A brief overview of our motivation is shown in Figure <ref type="figure" coords="2,171.42,398.32,3.74,8.64">1</ref>.</s></p><p><s coords="2,62.07,410.34,181.90,8.64">Our contributions are summarized as follows:</s></p><p><s coords="2,62.07,422.03,116.87,8.96;2,178.94,420.45,4.08,6.12;2,178.94,426.66,2.82,6.12;2,186.77,422.35,99.59,8.64;2,50.11,433.98,146.52,8.96;2,196.63,432.41,4.08,6.12;2,196.63,438.62,2.82,6.12;2,203.27,434.30,83.09,8.64;2,50.11,446.26,41.96,8.64">(1) We propose Getis-Ord G * i pooling, a novel pooling method based on spatial Getis-Ord G * i analysis of CNN feature maps.</s><s coords="2,96.54,445.94,50.63,8.96;2,147.16,444.36,4.08,6.12;2,147.16,450.57,2.82,6.12;2,154.69,446.26,131.67,8.64;2,50.11,458.21,236.25,8.64;2,50.11,470.17,25.19,8.64">Getis-Ord G * i pooling is shown to significantly improve model generalization for overhead image segmentation.</s></p><p><s coords="2,62.07,482.18,224.30,8.64;2,50.11,494.14,236.25,8.64;2,50.11,506.09,236.25,8.64;2,50.11,518.05,44.54,8.64">(2) We establish more generally that using geospatial knowledge in the design of CNNs can improve the generalizability of models which provides the simulated process of the data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." coords="2,50.11,540.36,83.11,10.75">Related Work</head><p><s coords="2,50.11,560.66,236.25,9.03;2,50.11,573.01,236.25,8.64;2,50.11,584.96,172.32,8.64">Semantic segmentation Fully connected neural networks (FCN) were recently proposed to improve the semantic segmentation of non-overhead imagery <ref type="bibr" coords="2,203.35,584.96,15.27,8.64" target="#b18">[20]</ref>.</s><s coords="2,231.52,584.96,54.84,8.64;2,50.11,596.92,236.25,8.64;2,50.11,608.87,109.22,8.64">Various techniques have been proposed to boost their performance, such as atrous convolution <ref type="bibr" coords="2,138.58,608.87,8.30,8.64" target="#b5">[6]</ref><ref type="bibr" coords="2,146.88,608.87,4.15,8.64" target="#b6">[7]</ref><ref type="bibr" coords="2,151.03,608.87,8.30,8.64" target="#b7">[8]</ref><ref type="bibr" coords="2,160.94,608.87,11.83,8.64" target="#b37">39]</ref>, skip connections <ref type="bibr" coords="2,249.83,608.87,15.27,8.64" target="#b24">[26]</ref>, and preserving max pooling index for unpooling <ref type="bibr" coords="2,234.57,620.83,10.58,8.64" target="#b2">[3]</ref>.</s><s coords="2,160.94,608.87,11.83,8.64">And recently, video is used to scale up training sets by synthesizing new training samples which is able to improve the accuracy of semantic segmentation networks <ref type="bibr" coords="2,210.75,656.69,15.27,8.64" target="#b39">[41]</ref>.</s><s coords="2,172.76,608.87,3.94,8.64">Remote sensing research has been driven largely by adapting advances in regular image analysis to overhead imagery.</s><s coords="2,179.76,608.87,16.61,8.64">In particular, deep learning approaches to overhead image analysis have become a standard practice for a variety of tasks, such as land use/land cover classification <ref type="bibr" coords="2,475.22,75.48,15.27,8.64" target="#b15">[17]</ref>, building extraction <ref type="bibr" coords="2,341.80,87.43,15.27,8.64" target="#b34">[36]</ref>, road segmentation <ref type="bibr" coords="2,439.09,87.43,15.27,8.64" target="#b21">[23]</ref>, car detection <ref type="bibr" coords="2,514.39,87.43,10.58,8.64" target="#b8">[9]</ref>, etc.</s><s coords="2,199.30,608.87,54.35,8.64">More literature can be found in a recent survey <ref type="bibr" coords="2,503.85,99.39,15.27,8.64" target="#b38">[40]</ref>.</s><s coords="2,253.65,608.87,11.45,8.64">And various segmentation networks have been proposed, such relation-augmentation networks <ref type="bibr" coords="2,437.84,123.30,16.60,8.64" target="#b22">[24]</ref> and casnet <ref type="bibr" coords="2,500.57,123.30,15.27,8.64" target="#b17">[19]</ref>.</s><s coords="2,265.10,608.87,21.26,8.64">However, these methods only adapt deep learning techniques and networks from regular to overhead images-they do not incorporate geographic structure or knowledge.</s><s coords="2,50.11,620.83,42.05,8.64">Knowledge guided neural networks Analyzing overhead imagery is not just a computer vision problem since principles of the physical world such as geo-spatial relationships can help.</s><s coords="2,95.63,620.83,17.16,8.64">For example, knowing the road map of a city can definitely improve tasks like building extraction or land cover segmentation.</s><s coords="2,116.25,620.83,30.45,8.64">While there are no works directly related to ours, there have been some initial attempts to incorporate geographic knowledge into deep learning <ref type="bibr" coords="2,517.14,256.92,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="2,529.34,256.92,11.83,8.64" target="#b36">38]</ref>.</s><s coords="2,150.16,620.83,21.99,8.64">Chen et al. <ref type="bibr" coords="2,359.97,268.87,11.62,8.64" target="#b4">[5]</ref> develop a knowledge-guided golf course detection approach using a CNN fine-tuned on temporally augmented data.</s><s coords="2,175.61,620.83,11.62,8.64">They also apply area-based rules during a post-processing step.</s><s coords="2,190.69,620.83,57.98,8.64">Zhang et al. <ref type="bibr" coords="2,461.03,304.74,16.60,8.64" target="#b36">[38]</ref> propose searching for adjacent parallel line segments as prior spatial information for the fast detection of runways.</s><s coords="2,254.68,620.83,17.16,8.64">However, these methods simply fuse prior knowledge from other sources.</s><s coords="2,275.31,620.83,11.06,8.64">Our proposed method is novel in that we incorporate geospatial rules into the CNN mechanics.</s><s coords="2,50.11,632.78,22.45,8.64">We show later how this helps regularize the model learning and leads to better generalization.</s><s coords="2,72.56,632.78,3.74,8.64">Pooling functions There are various studies in pooling for image classification as well as segmentation.</s><s coords="2,79.30,632.78,31.68,8.64">L p norm is proposed to extend max pooling where intermediate pooling functions are manually selected between max and average pooling to better fit the distribution of the input data.</s><s coords="2,113.88,632.78,18.26,8.64"><ref type="bibr" coords="2,528.52,449.93,16.60,8.64" target="#b16">[18]</ref> generalizes pooling methods by using a learned linear combination of max and average pooling.</s><s coords="2,135.04,632.78,7.75,8.64">Detail-Preserving Pooling (DPP) <ref type="bibr" coords="2,373.61,485.80,16.60,8.64" target="#b25">[27]</ref> learns weighted summations of pixels over different pooling regions.</s><s coords="2,145.69,632.78,19.92,8.64">Salient pixels are more importance in order to achieve higher visual satisfaction.</s><s coords="2,168.51,632.78,9.96,8.64">Stride convolution is used toreplace all max pooling layers and activation functions in a small classification model that is trained from scratch and achieve better performance <ref type="bibr" coords="2,526.03,545.57,15.27,8.64" target="#b28">[30]</ref>.</s><s coords="2,181.37,632.78,30.99,8.64">However, stride convolutions are common in segmentation tasks.</s><s coords="2,215.26,632.78,14.94,8.64">For example, the DeepLab series of networks <ref type="bibr" coords="2,524.29,569.48,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,536.82,569.48,8.30,8.64" target="#b7">8]</ref> use stride convolutional layers for feature down-sampling rather than max pooling.</s><s coords="2,233.11,632.78,9.96,8.64">To enhance detail preservation in segmentation, a recent polynomial pooling approach is proposed in <ref type="bibr" coords="2,343.78,617.31,15.27,8.64" target="#b33">[35]</ref>.</s><s coords="2,245.97,632.78,40.40,8.64;2,50.11,644.74,12.73,8.64">However, all these pooling methods are based on non-spatial statistics.</s><s coords="2,65.06,644.74,16.35,8.64">We instead incorporate geo-spatial rules/simulation to perform the downsampling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." coords="2,308.86,664.51,56.45,10.75">Methods</head><p><s coords="2,320.82,680.60,224.30,8.64;2,308.86,692.56,215.68,8.64">In this section, we investigate how geo-spatial knowledge can be incorporated into standard deep CNNs.</s><s coords="2,532.08,692.56,13.03,8.64;2,308.86,704.52,236.25,8.64;3,50.11,387.38,113.68,8.64">We discuss some general rules from geography to describe geo- spatial patterns on the Earth.</s><s coords="3,166.87,387.38,119.49,8.64;3,50.11,399.02,25.95,8.96;3,76.06,397.44,4.08,6.12;3,76.06,403.65,2.82,6.12;3,83.27,399.34,203.10,8.64;3,50.11,411.29,138.18,8.64">Then we propose using Getis-Ord G * i analysis, a common technique for geo-spatial clustering, to encapsulate these rules.</s><s coords="3,195.57,411.29,90.80,8.64;3,50.11,423.25,236.25,8.64;3,50.11,435.20,112.91,8.64">This then informs our pooling function which is very general and can be used in many network architectures.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." coords="3,50.11,456.64,77.24,9.85;3,127.35,455.82,4.08,6.12">Getis-Ord G *</head><p><s coords="3,127.35,462.03,2.82,6.12;3,134.67,456.64,89.64,9.85">i pooling (G-pooling)</s></p><p><s coords="3,62.07,476.51,224.30,8.64;3,50.11,488.28,236.25,8.82;3,50.11,500.24,203.45,8.82">We take inspiration from the well-known first law of geography: everything is related to everything else, but near things are more related than distant things <ref type="bibr" coords="3,234.48,500.42,15.27,8.64" target="#b29">[31]</ref>.</s><s coords="3,262.01,500.42,24.35,8.64;3,50.11,512.37,236.25,8.64;3,50.11,524.33,236.25,8.64;3,50.11,536.28,124.91,8.64">While this rule is very general and abstract, it motivates a number of quantitative frameworks that have been shown to improve geospatial data analysis.</s><s coords="3,181.57,536.28,104.80,8.64;3,50.11,548.24,236.25,8.64;3,50.11,560.19,102.38,8.64">For example, it motivates spatial autocorrelation which is the basis for spatial prediction models like kriging.</s><s coords="3,160.08,560.19,126.29,8.64;3,50.11,572.15,236.25,8.64;3,50.11,584.10,195.63,8.64">It also motivates the notion of spatial clustering wherein similar things that are spatially nearby are more significant than isolated things.</s><s coords="3,251.05,584.10,35.32,8.64;3,50.11,596.06,236.25,8.64;3,50.11,607.69,177.18,8.96;3,227.29,606.12,4.08,6.12;3,227.29,612.33,2.82,6.12;3,234.36,608.01,32.94,8.64">Our proposed framework exploits this to introduce a novel feature pooling method which we term Getis-Ord G * i pooling.</s><s coords="3,62.07,620.83,224.30,8.64;3,50.11,632.78,58.30,8.64">Pooling is used to spatially downsample the feature maps in deep CNNs.</s><s coords="3,111.39,632.78,174.97,8.64;3,50.11,644.74,236.25,8.64;3,50.11,656.69,236.25,8.64;3,50.11,668.65,84.05,8.64">In contrast to standard image downsampling methods which seek to preserve the spatial envelope of pixel values, pooling selects feature values that are more significant in some sense.</s><s coords="3,141.62,668.65,144.75,8.64;3,50.11,680.60,236.25,8.64;3,50.11,692.56,92.14,8.64">The most standard pooling method is max pooling in which the maximum feature value in a window is propagated.</s><s coords="3,148.30,692.56,138.06,8.64;3,50.11,704.51,39.01,8.64">Other pooling methods have been proposed.</s><s coords="3,92.14,704.51,194.22,8.64;3,308.86,75.48,136.60,8.64">Average pooling is an obvious choice and is used in <ref type="bibr" coords="3,318.76,75.48,17.01,8.64" target="#b12">[14,</ref><ref type="bibr" coords="3,335.77,75.48,12.76,8.64" target="#b35">37]</ref> for image classification.</s><s coords="3,448.44,75.48,96.68,8.64;3,308.86,87.43,78.05,8.64">Strided convolution <ref type="bibr" coords="3,528.52,75.48,16.60,8.64" target="#b14">[16]</ref> has also been used.</s><s coords="3,391.68,87.43,153.43,8.64;3,308.86,99.39,236.25,8.64;3,308.86,111.34,236.25,8.64;3,308.86,123.30,83.55,8.64">However, max pooling remains by far the most common as it has the intuitive appeal of extracting the maximum activation and thus the most prominent features of an image.</s></p><p><s coords="3,320.82,136.09,224.30,8.64;3,308.86,148.04,236.25,8.64;3,308.86,160.00,152.18,8.64">However, we postulate that isolated high feature values might not be the most informative and instead develop a method to propagate clustered values.</s><s coords="3,465.39,160.00,79.72,8.64;3,308.86,171.95,236.25,8.64;3,308.86,183.91,236.25,8.64;3,308.86,195.86,126.24,8.64">Specifically, we use a technique from geostatistics termed hotspot analysis to identify clusters of large values and then propagate a representative from these clusters.</s><s coords="3,441.42,195.86,103.69,8.64;3,308.86,207.50,51.43,8.96;3,360.29,205.93,4.08,6.12;3,360.29,212.13,2.82,6.12;3,368.63,207.82,176.49,8.64;3,308.86,219.77,236.25,8.64;3,308.86,231.73,98.64,8.64">Hotspot analysis uses the Getis-Ord G * i <ref type="bibr" coords="3,368.63,207.82,16.60,8.64" target="#b23">[25]</ref> statistic to find locations that have either high or low values and are surrounded by locations also with high or low values.</s><s coords="3,412.51,231.73,132.60,8.64;3,308.86,243.69,35.71,8.64">These locations are the so-called hotspots.</s><s coords="3,349.29,243.37,69.24,8.96;3,418.53,241.79,4.08,6.12;3,418.53,248.00,2.82,6.12;3,426.15,243.69,118.96,8.64;3,308.86,255.64,236.25,8.64;3,308.86,267.60,208.97,8.64">The Getis-Ord G * i statistic is computed by comparing the local sum of a feature and its neighbors proportionally to the sum of all features in a spatial region.</s><s coords="3,521.32,267.60,23.79,8.64;3,308.86,279.55,236.25,8.64;3,308.86,291.51,236.25,8.64;3,308.86,303.14,208.34,8.96;3,517.20,301.57,4.08,6.12;3,517.20,307.78,2.82,6.12;3,523.78,303.46,21.33,8.64;3,308.86,315.42,120.40,8.64">When the local sum is different from the expected local sum, and when that difference is too large to be the result of random noise, it will lead to a high positive or low negative G * i value that is statistically significant.</s><s coords="3,434.29,315.42,110.82,8.64;3,308.86,327.05,62.21,8.96;3,371.08,325.48,4.08,6.12;3,371.08,331.69,2.82,6.12;3,378.41,327.37,166.69,8.64;3,308.86,339.33,21.87,8.64">We focus on locations with high positive G * i values since we want to propagate activations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." coords="3,308.86,360.68,65.46,9.85">Definition</head><p><s coords="3,320.82,380.21,224.29,8.96">We now describe our G-pooling algorithm in detail.</s><s coords="3,308.86,392.48,135.00,8.64">Please see Figure <ref type="figure" coords="3,381.86,392.48,4.98,8.64" target="#fig_0">2</ref> for reference.</s><s coords="3,448.30,392.48,96.82,8.64;3,308.86,404.44,236.25,8.64;3,308.86,416.39,22.97,8.64">Similar to other pooling methods, we use a stride sliding window to downsample the input.</s><s coords="3,335.40,416.39,209.71,8.64;3,308.86,428.03,86.41,8.96;3,395.28,426.45,4.08,6.12;3,395.28,432.66,2.82,6.12;3,399.86,428.35,145.25,8.64;3,308.86,440.30,147.76,8.64">Given a feature map within the stride window, in order to compute its G * i , we first need to define the weight matrix based on the spatial locations.</s></p><p><s coords="3,320.82,453.09,224.30,8.64;3,308.86,464.70,236.25,9.68;3,308.86,477.00,139.61,8.64">We denote the feature values within the sliding window as X = x 1 , x 2 , ..., x n where n is the number of pixels (locations) within the sliding window.</s><s coords="3,451.53,477.00,93.59,8.64;3,308.86,488.64,134.69,8.96;3,443.55,487.07,4.08,6.12;3,443.55,493.27,2.82,6.12;3,451.04,488.96,94.08,8.64;3,308.86,500.91,49.21,8.64">We assume the window is rectangular and compute the G * i statistic at the center of the window.</s><s coords="3,363.72,500.59,165.80,9.65">Let the feature value at the center be x i .</s><s coords="3,535.16,500.91,9.95,8.64;3,308.86,512.87,236.25,8.64;3,308.86,524.51,166.66,9.65">(If the center does not fall on a pixel location then we compute x i as the average of the adjacent values.)</s><s coords="3,480.52,524.51,26.44,8.96;3,506.96,522.93,4.08,6.12;3,506.96,529.14,2.82,6.12;3,514.67,524.82,30.45,8.64;3,308.86,536.78,236.25,8.64;3,308.86,548.73,55.33,8.64">The G * i statistic uses weighed averages where the weights are based on spatial distances.</s><s coords="3,370.57,548.42,21.87,8.96;3,392.45,546.84,4.52,6.12;3,397.46,548.42,44.21,9.65;3,441.67,546.84,4.04,6.12;3,446.46,548.42,98.66,9.65;3,308.86,560.37,194.72,9.65">Let p x (x j ) and p y (x j ) denote the x and y positions of feature value x j in the image plane.</s><s coords="3,507.90,560.69,37.22,8.64;3,308.86,572.33,236.25,8.96;3,308.86,584.28,236.25,9.65;3,308.86,596.56,113.20,8.64">A weight matrix w that measures the Euclidean distance on the image plane between x i and the other locations within the sliding window is then computed as</s></p><formula xml:id="formula_0" coords="3,315.20,633.43,229.91,10.49">w i,j = (p x (x i ) − p x (x j )) 2 + (p y (x i ) − p y (x j )) 2 . (1)</formula><p><s coords="3,308.86,660.31,68.15,8.96;3,377.01,658.74,4.08,6.12;3,377.01,664.95,2.82,6.12;3,384.08,660.31,152.63,8.96">The Getis-Ord G * i value at location i is now computed as</s></p><formula xml:id="formula_1" coords="3,352.08,680.74,149.81,35.35">G * i = n j=1 w i,j x j − X n j=1 w i,j S [n n j=1 w 2 i,j −( n j=1 wi,j ) 2 ] n−1 .</formula><p><s coords="3,533.50,692.26,11.62,8.64;4,50.11,217.17,115.79,11.47">(2) where X and S are as below,</s></p><formula xml:id="formula_2" coords="4,139.18,239.25,147.18,26.77">X = n j=1 x j n ,<label>(3)</label></formula><formula xml:id="formula_3" coords="4,115.67,289.60,166.82,26.77">S = n j=1 x 2 j n − ( X) 2 . (<label>4</label></formula><formula xml:id="formula_4" coords="4,282.49,301.12,3.87,8.64">)</formula><p><s coords="4,62.07,328.24,192.71,8.96;4,254.77,326.67,4.08,6.12;4,254.77,332.87,2.82,6.12;4,262.54,328.56,23.82,8.64">Spatial clusters can be detected based on the G * i value.</s><s coords="4,50.11,340.51,236.25,8.64">The higher the value, the more significant the cluster is.</s><s coords="4,50.11,352.15,62.58,8.96;4,112.69,350.58,4.08,6.12;4,112.69,356.78,2.82,6.12;4,119.55,352.47,166.80,8.64;4,50.11,364.42,70.97,8.64">However, the G * i value just indicates whether there is a spatial cluster or not.</s><s coords="4,124.86,364.42,161.50,8.64;4,50.11,376.38,236.25,8.64;4,50.11,388.33,106.90,8.64">To achieve our goal of pooling, we need to summarize the local region of the feature map by extracting a representative value.</s><s coords="4,163.07,388.33,123.30,8.64">We use a threshold to do this.</s><s coords="4,50.11,399.97,75.97,8.96;4,126.09,398.40,4.08,6.12;4,126.09,404.60,2.82,6.12;4,134.01,400.29,152.35,8.64;4,50.11,411.93,236.25,9.65;4,50.11,424.20,223.35,8.64">If the computed G * i is greater than or equal to the threshold, a spatial cluster is detected and the value x i is used for pooling, otherwise the maximum in the window is used.</s></p><formula xml:id="formula_5" coords="4,61.34,459.09,225.02,26.67">G − pooling(x) = x i if G * i ≥ threshold max(x) if G * i &lt; threshold<label>(5)</label></formula><p><s coords="4,50.11,499.88,69.12,8.96;4,119.23,498.31,4.08,6.12;4,119.23,504.52,2.82,6.12;4,127.63,500.20,85.34,8.64">It's noted that G * i is in range [-2.8,2.8]</s><s coords="4,216.80,500.20,69.57,8.64;4,50.11,512.16,236.25,8.64;4,50.11,524.11,236.25,8.64;4,50.11,536.07,28.50,8.64">where a negative value indicates a coldspot which means a spatial scatter and a positive value indicates a hotspot which means a spatial cluster.</s><s coords="4,83.06,535.75,89.46,8.96;4,172.51,534.17,4.08,6.12;4,172.51,540.38,2.82,6.12;4,177.09,535.75,109.27,8.96">The absolute value |G * i | indicates the significance.</s><s coords="4,50.11,547.70,124.37,8.96;4,174.48,546.13,4.08,6.12;4,174.48,552.34,2.82,6.12;4,181.74,548.02,104.63,8.64;4,50.11,559.98,143.24,8.64">For example, a high positive G * i value indicates the feature is more likely to be a spatial cluster.</s></p><p><s coords="4,62.07,572.69,224.29,8.96;4,50.11,584.96,236.25,8.64;4,50.11,596.92,98.74,8.64">The output feature map produced by G-pooling is Gpooling(X) which results after sliding the window over the entire input feature map.</s><s coords="4,152.75,596.92,133.62,8.64;4,50.11,608.87,130.26,8.64">The threshold is set to 3 different values in this work, 1.0, 1.5, 2.0.</s><s coords="4,183.59,608.87,102.77,8.64;4,50.11,620.83,236.25,8.64;4,50.11,632.78,226.14,8.64">A higher threshold means the current feature map has less chance to be reported as a spatial cluster and so max pooling will be applied instead.</s><s coords="4,279.17,632.78,7.19,8.64;4,50.11,644.74,236.25,8.64;4,50.11,656.69,168.79,8.64">A lower threshold causes more spatial clusters to be detected and max pooling will be applied less often.</s><s coords="4,221.85,656.69,64.52,8.64;4,50.11,668.65,236.25,8.64;4,50.11,680.60,66.83,8.64">As the threshold ranges from 1.0 to 1.5 to 2.0, fewer spatial clusters/hotspots will be detected.</s><s coords="4,121.91,680.60,164.45,8.64;4,50.11,692.56,236.25,8.64;4,50.11,704.51,20.75,8.64">We find that a threshold of 2.0 results in few hostpots being detected and max pooling mostly to be used.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3." coords="4,308.86,74.40,121.83,9.85">Network Architecture</head><p><s coords="4,320.82,97.94,224.30,8.64;4,308.86,109.90,26.29,8.64">A pretrained VGG network <ref type="bibr" coords="4,436.66,97.94,16.60,8.64" target="#b27">[29]</ref> is used in our experiments.</s><s coords="4,340.32,109.90,204.79,8.64;4,308.86,121.85,236.25,8.64;4,308.86,133.81,97.26,8.64">VGG has been widely used as a backbone in various semantic segmentation networks such as FCN <ref type="bibr" coords="4,512.70,121.85,15.27,8.64" target="#b18">[20]</ref>, Unet <ref type="bibr" coords="4,323.10,133.81,15.27,8.64" target="#b24">[26]</ref>, and SegNet <ref type="bibr" coords="4,392.02,133.81,10.58,8.64" target="#b2">[3]</ref>.</s><s coords="4,409.07,133.81,136.05,8.64;4,308.86,145.76,163.56,8.64">In VGG, the standard max pooling is a 2×2 window size with a stride of 1.</s><s coords="4,477.17,145.44,67.94,8.96;4,308.86,157.72,236.25,8.64;4,308.86,169.67,236.25,8.64;4,308.86,181.31,236.25,8.96">Our proposed Gpooling uses a 4×4 window size with a stride of 4. Therefore, after applying the standard pooling, the size of feature map drops to 1/2, while with our G-pooling it drops to 1/4.</s><s coords="4,308.86,193.27,236.25,8.96;4,308.86,205.22,74.86,8.96;4,383.72,203.65,4.08,6.12;4,383.72,209.85,2.82,6.12;4,391.66,205.54,153.45,8.64;4,308.86,217.49,52.31,8.64">A small window size is not used in our proposed G-pooling since Getis-Ord G * i analysis may not work well in such a small region.</s><s coords="4,365.56,217.49,179.56,8.64;4,308.86,229.13,236.25,8.96;4,308.86,241.40,236.25,8.64;4,308.86,253.04,112.52,8.96">However, we tested the scenario where standard pooling is performed with a 4 × 4 sliding window and the performance is only slightly different from that using the standard 2 × 2 window.</s><s coords="4,426.67,253.36,118.44,8.64;4,308.86,265.32,236.25,8.64;4,308.86,277.27,26.28,8.64">In general, segmentation networks using VGG16 as the backbone have 5 max pooling layers.</s><s coords="4,342.71,277.27,202.40,8.64;4,308.86,288.91,236.25,8.96;4,308.86,301.18,59.22,8.64">So, when we replace max pooling with our proposed G-pooling, there will be two G-pooling and one max pooling layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." coords="4,308.86,336.92,77.04,10.75">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1." coords="4,308.86,361.07,54.48,9.85">Dataset</head><p><s coords="4,308.86,384.23,236.25,9.03;4,308.86,396.57,236.25,8.64;4,308.86,408.53,14.11,8.64">ISPRS dataset We evaluate our method on two image datasets from the ISPRS 2D Semantic Labeling Challenge <ref type="bibr" coords="4,308.86,408.53,10.58,8.64" target="#b0">[1]</ref>.</s><s coords="4,328.82,408.53,216.30,8.64;4,308.86,420.48,236.25,8.64;4,308.86,432.44,36.81,8.64">These datasets are comprised of very high resolution aerial images over two cities in Germany: Vaihingen and Potsdam.</s><s coords="4,348.73,432.44,196.38,8.64;4,308.86,444.39,236.25,8.64;4,308.86,456.35,236.25,8.64;4,308.86,468.30,185.06,8.64">While Vaihingen is a relatively small village with many detached buildings and small multi-story buildings, Potsdam is a typical historic city with large building blocks, narrow streets and dense settlement structure.</s><s coords="4,499.31,468.30,45.81,8.64;4,308.86,480.26,236.25,8.64;4,308.86,492.21,236.25,8.64">The goal is to perform semantic labeling of the images using six common land cover classes: buildings, impervious surfaces (e.g.</s><s coords="4,308.86,504.17,236.25,8.64">roads), low vegetation, trees, cars and clutter/background.</s><s coords="4,308.86,516.12,236.25,8.64">We report test metrics obtained on the held-out test images.</s></p><p><s coords="4,308.86,532.22,236.25,9.03;4,308.86,544.25,236.25,8.96">Vaihingen The Vaihingen dataset has a resolution of 9 cm/pixel with tiles of approximately 2100 × 2100 pixels.</s><s coords="4,308.86,556.52,236.25,8.64;4,308.86,568.48,21.31,8.64">There are 33 images, from which 16 have a public ground truth.</s><s coords="4,334.94,568.48,210.17,8.64;4,308.86,580.43,236.25,8.64;4,308.86,592.39,236.25,8.64">Even though the tiles consist of Infrared-Red-Green (IRRG) images and DSM data extracted from the Lidar point clouds, we use only the IRRG images in our work.</s><s coords="4,308.86,604.34,236.25,8.64;4,308.86,616.30,226.56,8.64">We select five images for validation (IDs: 11, 15, 28, 30 and 34) and the remaining 11 for training, following <ref type="bibr" coords="4,502.58,616.30,15.77,8.64" target="#b20">[22,</ref><ref type="bibr" coords="4,519.65,616.30,11.83,8.64" target="#b26">28]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="4,308.86,632.39,36.88,8.96">Potsdam</head><p><s coords="4,352.13,632.78,192.99,8.64;4,308.86,644.42,176.35,8.96">The Potsdam dataset has a resolution of 5 cm/pixel with tiles of 6000 × 6000 pixels.</s><s coords="4,492.19,644.74,52.93,8.64;4,308.86,656.69,193.77,8.64">There are 38 images, from which 24 have public ground truth.</s><s coords="4,505.67,656.69,39.44,8.64;4,308.86,668.65,168.83,8.64">Similar to Vaihingen, we only use the IRRG images.</s><s coords="4,481.58,668.65,63.53,8.64;4,308.86,680.60,236.25,8.64;4,308.86,692.56,236.25,8.64;4,308.86,704.51,48.06,8.64">We select seven images for validation (IDs: 2 11, 2 12, 4 10, 5 11, 6 7, 7 8 and 7 10) and the remaining 17 for training, again following <ref type="bibr" coords="4,324.08,704.51,15.77,8.64" target="#b20">[22,</ref><ref type="bibr" coords="4,341.15,704.51,11.83,8.64" target="#b26">28]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." coords="5,50.11,353.74,121.15,9.85">Experimental settings</head><p><s coords="5,50.11,373.75,236.25,9.03;5,50.11,386.10,236.25,8.64;5,50.11,397.73,236.25,8.96">Baselines Here, we compare our proposed G-pooling with the standard max-pooling, average-pooling, stride convolution, and the recently proposed P-pooling <ref type="bibr" coords="5,267.28,398.05,15.27,8.64" target="#b33">[35]</ref>.</s><s coords="5,50.11,410.01,236.25,8.64;5,50.11,421.96,236.25,8.64;5,50.11,433.92,21.86,8.64">Max/average pooling is commonly for downsampling in the semantic segmentation networks that have VGG as a backbone.</s><s coords="5,77.48,433.92,208.89,8.64;5,50.11,445.87,96.77,8.64">ResNet <ref type="bibr" coords="5,110.10,433.92,16.60,8.64" target="#b10">[11]</ref> is proposed without using any pooling but strided convolution.</s><s coords="5,153.92,445.87,132.44,8.64;5,50.11,457.83,236.25,8.64;5,50.11,469.78,236.25,8.64">Such a network architecture has been adopted by recent studies for semantic segmentation, in particular the DeepLab series <ref type="bibr" coords="5,187.97,469.78,8.63,8.64" target="#b5">[6]</ref><ref type="bibr" coords="5,196.61,469.78,4.32,8.64" target="#b6">[7]</ref><ref type="bibr" coords="5,200.92,469.78,8.63,8.64" target="#b7">[8]</ref> and PSPNet <ref type="bibr" coords="5,267.28,469.78,15.27,8.64" target="#b37">[39]</ref>.</s><s coords="5,50.11,481.74,236.25,8.64;5,50.11,493.69,236.25,8.64;5,50.11,505.65,194.91,8.64">Max pooling is removed and instead strided convolution is used to downsample the feature maps while dilated convolution is used to enlarge the receptive fields.</s><s coords="5,252.52,505.65,33.84,8.64;5,50.11,517.60,236.25,8.64;5,50.11,529.24,102.65,8.96">There is also work on detail preserving pooling, for example DDP <ref type="bibr" coords="5,50.11,529.56,16.60,8.64" target="#b25">[27]</ref> and P-pooling <ref type="bibr" coords="5,133.67,529.56,15.27,8.64" target="#b33">[35]</ref>.</s><s coords="5,159.44,529.56,126.92,8.64;5,50.11,541.20,236.25,8.96;5,50.11,553.47,99.34,8.64">We select the most recent one, P-pooling, which outperforms the other detail preserving methods for comparison.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." coords="5,50.11,576.51,108.87,9.85">Evaluation Metrics</head><p><s coords="5,62.07,596.92,224.30,8.64;5,50.11,608.87,236.25,8.64">We have two goals in this work, the model's segmentation accuracy and its generalization performance.</s><s coords="5,50.11,620.83,236.25,8.64;5,50.11,632.78,236.25,8.64;5,50.11,644.74,77.79,8.64">Model accuracy is used to report the performance on the test/validation set using the model trained with training set within one dataset.</s><s coords="5,135.19,644.74,151.17,8.64;5,50.11,656.69,236.25,8.64;5,50.11,668.65,30.16,8.64">Model generalizability is used to report the performance of the test/validation set with another dataset.</s><s coords="5,90.15,668.65,196.21,8.64;5,50.11,680.60,210.88,8.64">In general, the domain gap between train and test/validation set from one dataset is relatively small.</s><s coords="5,263.93,680.60,22.44,8.64;5,50.11,692.56,236.25,8.64">However, cross-dataset testing exists large domain shift problem.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="5,308.86,354.42,68.33,8.96">Model accuracy</head><p><s coords="5,387.16,354.81,157.96,8.64;5,308.86,366.77,236.25,8.64;5,308.86,378.72,236.25,8.64;5,308.86,390.68,28.24,8.64">The commonly used per class intersection over union (IoU) and mean IoU (mIoU) as well as the pixel accuracy are adopted for evaluating segmentation accuracy.</s></p><p><s coords="5,308.86,412.61,236.25,9.03;5,308.86,424.95,236.25,8.64;5,308.86,436.91,236.25,8.64;5,308.86,448.87,156.42,8.64">Model generalizability Specifically, we will perform evaluation on the ISPRS Potsdam set with a model trained on the ISPRS Vahingen set (Potsdam→Vaihingen) and reverse the order (Vaihingen→Potsdam).</s><s coords="5,469.45,448.87,75.67,8.64;5,308.86,460.82,236.25,8.64;5,308.86,472.78,27.31,8.64">Pixel accuracy and mIoU are used to report the performance of the generalizability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4." coords="5,308.86,494.77,129.06,9.85">Implementation Details</head><p><s coords="5,308.86,514.43,236.25,9.03;5,308.86,526.78,129.54,8.64">Implementation of G-pooling Models are implemented using the PyTorch framework.</s><s coords="5,450.44,526.78,94.67,8.64;5,308.86,538.73,236.25,8.64;5,308.86,550.37,137.14,8.96">Max-pooling, averagepooling, stride conv are provided as built-in function and P-pooling has open-source code.</s><s coords="5,453.99,550.37,91.13,8.96;5,308.86,562.64,236.25,8.64;5,308.86,574.60,83.21,8.64">We implement our Gpooling in C and use the interface to connect to PyTorch for network training.</s><s coords="5,396.01,574.60,149.10,8.64;5,308.86,586.55,236.25,8.64">We adopt the network architecture of FCN <ref type="bibr" coords="5,331.92,586.55,16.60,8.64" target="#b18">[20]</ref> with a backbone of a pretrained VGG-16 <ref type="bibr" coords="5,526.03,586.55,15.27,8.64" target="#b27">[29]</ref>.</s><s coords="5,308.86,598.19,236.25,8.96;5,308.86,610.46,47.61,8.64">The details of the FCN using our G-pooling can be found in Section 3.3.</s><s coords="5,360.43,610.46,184.69,8.64;5,308.86,622.42,104.99,8.64">The results in Table <ref type="table" coords="5,442.69,610.46,4.98,8.64" target="#tab_0">1</ref> are reported using FCN with a VGG-16 backbone.</s></p><p><s coords="5,308.86,644.35,236.25,9.03;5,308.86,656.69,236.25,8.64;5,308.86,668.65,236.25,8.64;5,308.86,680.60,71.48,8.64">Training settings Since the image tiles are too large to be fed through a deep CNN due to limited GPU memory, we randomly extract image patches of size of 256×256 pixels as the training set.</s><s coords="5,383.32,680.60,161.79,8.64;5,308.86,692.56,236.25,8.64;5,308.86,704.51,33.48,8.64">Following standard practice, we only use horizontal and vertical flipping as data augmentation during training.</s><s coords="5,345.33,704.20,199.79,8.96;6,50.11,300.18,117.72,8.64">For testing, the whole image is split into 256×256 patches with a stride of 256.</s><s coords="6,174.15,300.18,112.22,8.64;6,50.11,312.13,159.12,8.64">Then, the predictions of all patches are concatenated for evaluation.</s><s coords="6,62.07,324.72,224.30,8.64;6,50.11,336.67,236.25,8.64;6,50.11,348.63,236.25,8.64">We train all our models using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1, a momentum of 0.9, a weight decay of 0.0005 and a batch size of 5.</s><s coords="6,50.11,360.58,236.25,8.64;6,50.11,372.54,117.87,8.64">If the validation loss plateaus for 3 consecutive epochs, we divide the learning rate by 10.</s><s coords="6,170.97,372.54,115.40,8.64;6,50.11,384.49,236.25,8.64;6,50.11,396.45,123.20,8.64">If the validation loss plateaus for 6 consecutive epochs or the learning rate is less than 1e-8, we stop the model training.</s><s coords="6,178.96,396.45,107.41,8.64;6,50.11,408.41,115.94,8.64">We use a single TITAN V GPU for training and testing.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." coords="6,50.11,539.09,143.73,10.75">Effectiveness of G-pooling</head><p><s coords="6,62.07,560.42,224.30,8.64;6,50.11,572.38,236.25,8.64;6,50.11,584.33,189.00,8.64">In this section, we first show that incorporating geospatial knowledge into a pooling function of the standard CNN learning can improve segmentation accuracy.</s><s coords="6,249.38,584.33,36.98,8.64;6,50.11,596.29,236.25,8.64;6,50.11,607.92,81.78,8.96">Then we demonstrate the promising generalization capability of our proposed G-pooling.</s></p><p><s coords="6,62.07,620.83,224.30,8.64;6,50.11,632.78,236.25,8.64">The segmentation accuracy on FCN using various pooling functions reported on the test set is shown in Table <ref type="table" coords="6,278.89,632.78,3.74,8.64" target="#tab_0">1</ref>.</s><s coords="6,50.11,644.42,236.25,8.96;6,50.11,656.69,103.92,8.64">For G-pooling, we experiment on 3 different thresholds, which is 1.0, 1.5 and 2.0.</s><s coords="6,159.25,656.37,63.37,8.96;6,222.62,654.80,4.08,6.12;6,222.62,661.01,2.82,6.12;6,230.40,656.69,55.96,8.64;6,50.11,668.65,18.26,8.64">The range of G * i value is [-2.8, 2.8].</s><s coords="6,71.28,668.33,147.35,8.96;6,218.64,666.76,4.08,6.12;6,218.64,672.96,2.82,6.12;6,225.17,668.65,61.19,8.64;6,50.11,680.60,109.20,8.64">As explained in Section 3.2, higher G * i value can cause more uses of max pooling.</s><s coords="6,164.81,680.29,62.51,8.96;6,227.32,678.71,4.08,6.12;6,227.32,684.92,2.82,6.12;6,235.20,680.60,51.17,8.64;6,50.11,692.56,156.45,8.64">If we set the G * i value as 2.8, then the case will be all max pooling.</s><s coords="6,212.90,692.56,73.46,8.64;6,50.11,704.51,88.51,8.64">Qualitative results are shown in Figure <ref type="figure" coords="6,131.15,704.51,3.74,8.64" target="#fig_2">4</ref>.</s><s coords="6,141.66,704.51,144.71,8.64;6,308.86,300.18,236.25,8.64;6,308.86,312.13,92.20,8.64">And the quantitative results for eval-uating model accuracy and cross-location generalization is shown in Table <ref type="table" coords="6,371.74,312.13,29.33,8.64" target="#tab_1">1 and 2</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,403.56,312.13,49.58,8.64">respectively.</head><p><s coords="6,308.86,328.61,236.25,9.03;6,308.86,340.95,236.25,8.64;6,308.86,352.91,155.26,8.64">Non-spatial vs geospatial statistics The baselines of pooling functions are usually non-spatial statistics, for example, finding the max/average value.</s><s coords="6,469.80,352.91,75.32,8.64;6,308.86,364.86,236.25,8.64;6,308.86,376.82,127.96,8.64">Our approach provides a geospatial process to simulate how things are related based on spatial location.</s><s coords="6,443.95,376.82,101.16,8.64;6,308.86,388.59,219.76,8.82">Here, we pose the question, "is the knowledge useful to train a deep CNN?".</s><s coords="6,534.04,388.77,11.07,8.64;6,308.86,400.73,236.25,8.64;6,308.86,412.68,236.25,8.64">As we mentioned in Section 3, such a knowledge incorporated method can bring the benefit of improved generalizability.</s><s coords="6,308.86,424.64,236.25,8.64;6,308.86,436.59,236.25,8.64;6,308.86,448.23,141.36,8.96">As shown in Table <ref type="table" coords="6,385.92,424.64,3.74,8.64" target="#tab_0">1</ref>, for Potsdam, using geospatial knowledge to design the pooling function can bring 1.23% improvement compared to P-pooling.</s><s coords="6,453.84,448.23,91.27,8.96;6,308.86,460.50,236.25,8.64;6,308.86,472.46,236.25,8.64;6,308.86,484.41,41.24,8.64">Our G-pooling-1.0 and 2.0 is not able to outperform some baselines in the model accuracy testing, which indicates the threshold selection is important.</s><s coords="6,355.40,484.41,189.72,8.64;6,308.86,496.37,117.52,8.64">Some classes of the baselines have higher performance compared to ours.</s><s coords="6,435.22,496.37,109.90,8.64;6,308.86,508.32,194.79,8.64">This is expected since the dataset is relatively small and may be overfitting.</s><s coords="6,506.72,508.32,38.40,8.64;6,308.86,519.96,236.25,8.96;6,308.86,532.23,107.20,8.64">The qualitative results in Figure <ref type="figure" coords="6,403.30,520.28,4.98,8.64" target="#fig_2">4</ref> show our proposed G-pooling has less pepper-and-salt effect.</s><s coords="6,420.02,532.23,125.09,8.64;6,308.86,544.19,210.08,8.64">In particular, there is less noise inside the objects compared to the other methods.</s><s coords="6,527.40,544.19,17.71,8.64;6,308.86,555.82,236.25,8.96;6,308.86,568.10,236.25,8.64;6,308.86,580.05,58.92,8.64">This demonstrates our proposed G-pooling simulates the geospatial distributions and makes the prediction within the objects more compact.</s><s coords="6,370.78,580.05,174.34,8.64;6,308.86,592.01,236.25,8.64;6,308.86,603.96,93.24,8.64">The effects of threshold is shown in Table <ref type="table" coords="6,537.64,580.05,3.74,8.64" target="#tab_2">3</ref>, as described in Section 3, the higher the threshold the less spatial cluster detected.</s><s coords="6,308.86,632.78,236.25,8.64;6,308.86,644.74,135.87,8.64"><ref type="table" coords="6,308.86,632.78,4.98,8.64" target="#tab_1">2</ref> compares using pooling functions with using unsupervised domain adaptation (UDA).</s><s coords="6,449.11,644.74,96.01,8.64;6,308.86,656.69,236.25,8.64;6,308.86,668.65,236.25,8.64;6,308.86,680.60,189.08,8.64">We note that the UDA method AdaptSegNet <ref type="bibr" coords="6,401.34,656.69,16.60,8.64" target="#b30">[32]</ref> uses a large amount of unlabeled data from the target dataset to adapt the model which has been demonstrated to help generalization.</s><s coords="6,505.25,680.60,39.86,8.64;6,308.86,692.56,190.72,8.64">The other methods don't benefit from the unlabeled data.</s><s coords="6,505.01,692.56,40.11,8.64;6,308.86,704.20,236.25,8.96;7,50.11,75.48,130.00,8.64">As shown in Table <ref type="table" coords="6,345.62,704.51,3.74,8.64" target="#tab_1">2</ref>, our proposed G-pooling is able to achieve the best generalization performance.</s><s coords="7,183.15,75.48,103.21,8.64;7,50.11,87.11,215.91,8.96">For Potsdam→Vaihingen, G-pooling outperforms P-pooling by more than 2%.</s><s coords="7,272.68,87.43,13.69,8.64;7,50.11,99.39,236.25,8.64;7,50.11,111.34,94.89,8.64">For Vaihingen→Potsdam, the improvement is even more significant, at least 3.41%.</s><s coords="7,149.36,111.34,137.01,8.64;7,50.11,122.98,236.25,8.96;7,50.11,135.25,236.25,8.64;7,50.11,147.21,100.69,8.64">When we compare the knowledge incorporation method G-pooling with the domain adaptation method AdaptSegNet, the performance difference is just 0.61% for Potsdam.</s><s coords="7,158.55,147.21,127.81,8.64;7,50.11,159.16,236.25,8.64;7,50.11,171.12,19.65,8.64">The results verify our assumption that incorporating knowledge helps generalizations as well.</s><s coords="7,73.18,171.12,213.19,8.64;7,50.11,183.07,236.25,8.64;7,50.11,195.03,106.48,8.64">And the performance is close to that of domain adaptation which utilizes a great amount of unlabeled data to learn the data distribution.</s><s coords="7,161.75,195.03,124.61,8.64;7,50.11,206.98,236.25,8.64;7,50.11,218.94,236.25,8.64;7,50.11,230.89,58.93,8.64">Even though knowledge incorporation doesn't outperform data-based domain adaptation, these two methods can be combined to provide even better generalization.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="6,308.86,620.44,236.25,9.03">Domain adaptation vs knowledge incorporation Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." coords="7,50.11,253.24,210.04,10.75">G-pooling and state-of-the-art methods</head><p><s coords="7,62.07,273.68,224.30,8.96;7,50.11,285.95,236.25,8.64;7,50.11,297.91,236.25,8.64;7,50.11,309.54,113.26,8.96">In order to verify that our proposed G-pooling is able to improve state-of-the-art segmentation approaches, we select DeepLab <ref type="bibr" coords="7,107.01,297.91,11.62,8.64" target="#b5">[6]</ref> and SegNet <ref type="bibr" coords="7,171.19,297.91,11.62,8.64" target="#b2">[3]</ref> as additional network architectures to test G-pooling.</s><s coords="7,166.27,309.86,120.09,8.64;7,50.11,321.82,236.25,8.64;7,50.11,333.77,105.24,8.64">As mentioned above, the models in Section 5 use FCN as the network architecture and VGG-16 as the backbone.</s><s coords="7,160.36,333.77,126.01,8.64;7,50.11,345.73,236.25,8.64;7,50.11,357.68,16.88,8.64">For fair comparison with FCN, VGG-16 is also used as the backbone in DeepLab and Seg-Net.</s></p><p><s coords="7,62.07,369.70,224.30,8.64;7,50.11,381.66,49.49,8.64">DeepLab <ref type="bibr" coords="7,100.96,369.70,11.62,8.64" target="#b5">[6]</ref> uses a large receptive fields through dilated convolution.</s><s coords="7,107.30,381.48,179.07,8.82;7,50.11,393.44,236.25,8.82;7,50.11,405.57,236.25,8.64;7,50.11,417.21,143.64,8.96">For the baseline DeepLab itself, pool4 and pool5 from the backbone VGG-16 are removed and followed by <ref type="bibr" coords="7,88.11,405.57,16.60,8.64" target="#b30">[32]</ref> and the dilated conv layers with a dilation rate of 2 are replaced with conv5 layers.</s><s coords="7,196.95,417.21,89.42,8.96;7,50.11,429.16,236.25,8.96;7,50.11,441.26,25.19,8.59">For the G-pooling version, pool1,pool2 are replaced with G-pooling and we keep pool3.</s><s coords="7,78.71,441.44,207.65,8.64;7,50.11,453.07,236.25,8.96;7,50.11,465.35,87.33,8.64">Thus there are three max pooling layers in the baseline and one G-pooling layer and one max pooling layer in our proposed version.</s><s coords="7,141.86,465.35,144.51,8.64;7,50.11,477.30,236.25,8.64;7,50.11,489.26,71.86,8.64">SegNet uses an encoder-decoder architecture and preserves the max pooling index for unpooling in the decoder.</s><s coords="7,124.86,489.26,161.50,8.64;7,50.11,501.03,236.25,8.82;7,50.11,512.85,236.25,8.96;7,50.11,524.80,167.99,8.96">Similar to Deeplab, there are 5 max pooling layers in total in the encoder of SegNet so pool1,pool2 are replaced with the proposed G pool1 and pool3,pool4 are replaced with G pool2, and pool5 is kept.</s><s coords="7,222.64,525.12,63.72,8.64;7,50.11,536.76,236.25,8.96;7,50.11,548.71,236.25,8.96;7,50.11,560.67,236.25,8.96;7,50.11,572.94,31.67,8.64">This leads us to use a 4 × 4 unpooling window to recover the spatial resolution where the original ones are just 2 × 2. Thus there are two G-pooling and one max pooling layers in our SegNet version.</s></p><p><s coords="7,62.07,584.64,224.30,8.96;7,50.11,596.60,236.25,8.96;7,50.11,608.87,33.21,8.64">As can be seen in Table <ref type="table" coords="7,181.48,584.96,3.74,8.64" target="#tab_3">4</ref>, G-pooling is able to improve the model accuracy for Potsdam, 67.97% → 68.33%.</s><s coords="7,89.88,608.87,196.48,8.64;7,50.11,620.51,236.25,8.96;7,50.11,632.78,135.38,8.64">And the improvement on the generalization test Potsdam→Vaihingen is even more obvious, G-pooling improves mIoU from 38.57 to 40.04.</s><s coords="7,188.51,632.78,97.85,8.64;7,50.11,644.74,122.18,8.64">Similar observations can be made for SegNet and FCN.</s><s coords="7,175.12,644.74,111.24,8.64;7,50.11,656.69,236.25,8.64;7,50.11,668.65,67.30,8.64">For Vaihingen, even though the model accuracy is not as high as the baseline, the difference is small.</s><s coords="7,123.91,668.65,162.45,8.64;7,50.11,680.60,160.65,8.64">The mIoU of our versions of DeepLab, SegNet and FCN is less than 1% lower.</s><s coords="7,215.50,680.60,70.86,8.64;7,50.11,692.56,236.25,8.64;7,50.11,704.51,236.25,8.64;7,308.86,75.48,40.12,8.64">We note that Vaihingen is an easier dataset than Potsdam, since it only includes urban scenes while Potsdam includes both urban and nonurban.</s><s coords="7,351.96,75.48,193.16,8.64;7,308.86,87.11,101.23,8.96">However, the generalizability of our model using G-pooling is much better.</s><s coords="7,413.25,87.43,131.87,8.64;7,308.86,99.07,236.25,8.96;7,308.86,111.34,236.25,8.64;7,308.86,123.30,61.29,8.64">As shown, when testing Potsdam using a model trained on Vaihingen, FCN with G-pooling is able to achieve 23.02% mIoU which is an improvement of 7.54% IoU.</s><s coords="7,374.78,123.30,170.33,8.64;7,308.86,135.25,87.55,8.64">The same observations can be made for DeepLab and SegNet.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." coords="7,308.86,442.29,65.77,10.75">Discussion</head><p><s coords="7,320.82,465.41,224.30,8.64;7,308.86,477.37,66.81,8.64">Incorporating knowledge is not a novel approach for neural networks.</s><s coords="7,380.28,477.37,164.83,8.64;7,308.86,489.32,236.25,8.64;7,308.86,501.28,213.82,8.64">Before deep learning, there was work on rule-based neural networks which required expert knowledge to design the network for specific applications.</s><s coords="7,528.52,501.28,16.60,8.64;7,308.86,513.23,236.25,8.64;7,308.86,525.19,236.25,8.64">Due to the large capacity of deep models, deep learning has become the primary approach to address vision problems.</s><s coords="7,308.86,537.14,236.25,8.64;7,308.86,549.10,207.07,8.64">However, deep learning is a data-driven approach which relies significantly on the amount of training data.</s><s coords="7,522.61,549.10,22.51,8.64;7,308.86,561.05,236.25,8.64;7,308.86,573.01,105.04,8.64">If the model is trained with a large amount of data then it will have good generalization.</s><s coords="7,421.13,573.01,123.98,8.64;7,308.86,584.96,236.25,8.64;7,308.86,596.92,186.77,8.64">But the case is often, particularly in overhead image segmentation, that the dataset is not large enough like it is in ImageNet/Cityscapes.</s><s coords="7,498.86,596.92,46.26,8.64;7,308.86,608.87,43.71,8.64">This causes overfitting.</s><s coords="7,357.67,608.87,187.45,8.64;7,308.86,620.83,80.91,8.64">Early stopping, cross-validation, etc. can help to avoid overfitting.</s><s coords="7,396.95,620.83,148.16,8.64;7,308.86,632.78,236.25,8.64;7,308.86,644.74,19.65,8.64">Still, if domain shift exists between the training and test sets, the deep models do not perform well.</s><s coords="7,334.29,644.74,210.83,8.64;7,308.86,656.69,126.94,8.64">In this work, we propose a knowledge-incorporated approach to reduce overfitting.</s><s coords="7,443.85,656.69,101.27,8.64;7,308.86,668.65,236.25,8.64;7,308.86,680.60,236.25,8.64;7,308.86,692.56,82.74,8.64">We address the question of how to incorporate the knowledge directly into the deep models by proposing a novel pooling method for overhead image segmentation.</s><s coords="7,395.99,692.56,149.12,8.64;7,308.86,704.51,42.92,8.64">But some issues still need discussing as follows.</s><s coords="8,50.11,410.98,236.25,9.03;8,50.11,423.00,167.13,8.96;8,217.24,421.43,4.08,6.12;8,217.24,427.64,2.82,6.12;8,225.87,423.32,60.49,8.64;8,50.11,435.28,196.57,8.64">Scenarios using G-pooling As mentioned in section 3, Gpooling is developed using Getis-Ord G * i analysis which quantifies how the spatial convergence occurs.</s><s coords="8,257.08,435.28,29.29,8.64;8,50.11,447.23,236.25,8.64;8,50.11,459.19,22.97,8.64">This is a simulated process design for geospatial data downsampling.</s><s coords="8,76.72,459.19,209.64,8.64;8,50.11,471.14,34.03,8.64">Thus it's not necessarily appropriate for other image datasets.</s><s coords="8,87.09,471.14,199.27,8.64;8,50.11,483.10,45.96,8.64">This is more general restriction of incorporating of knowledge.</s><s coords="8,100.75,482.78,69.22,8.96;8,169.97,481.20,4.08,6.12;8,169.97,487.41,2.82,6.12;8,177.57,483.10,108.79,8.64;8,50.11,495.05,138.54,8.64">The Getis-Ord G * i provides a method to identify spatial clusters while training.</s><s coords="8,193.74,495.05,92.62,8.64;8,50.11,507.01,236.25,8.64;8,50.11,518.96,171.80,8.64">The effect is similar to conditional random fields/Markov random fields in standard computer vision post-processing methods.</s><s coords="8,228.57,518.96,57.79,8.64;8,50.11,530.92,236.25,8.64;8,50.11,542.87,236.25,8.64;8,50.11,554.83,236.25,8.64;8,50.11,566.78,74.72,8.64">However, it is different from them since the spatial clustering is dynamically changing based on the feature maps and the geospatial location while post-processing methods rely on the prediction of the models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="8,50.11,596.53,101.87,8.96">Local geospatial pattern</head><p><s coords="8,161.94,596.60,124.42,8.96;8,50.11,608.87,128.63,8.64">We now explain how G-pooling works in deep neural networks.</s><s coords="8,184.86,608.55,51.18,8.96;8,236.04,606.98,4.08,6.12;8,236.04,613.19,2.82,6.12;8,244.12,608.87,42.24,8.64;8,50.11,620.83,236.25,8.64;8,50.11,632.78,179.41,8.64">Getis-Ord G * i analysis is usually used to analyze a global region hotspot detection which describes the geospatial convergence.</s><s coords="8,235.13,632.78,51.24,8.64;8,50.11,644.42,236.25,8.96;8,50.11,656.69,65.89,8.64">As shown in Figure <ref type="figure" coords="8,79.69,644.74,3.74,8.64" target="#fig_1">3</ref>, G-pooling will be applied twice to downsample the feature map.</s><s coords="8,121.25,656.37,165.12,8.96;8,50.11,668.33,130.84,8.96">The spatial size of the G-pooling will be 64 × 64 and 16 × 16 respectively.</s><s coords="8,183.95,668.65,102.42,8.64;8,50.11,680.29,236.25,8.96;8,50.11,692.24,88.14,8.96">And the max-pooling will lead to the size of feature map being reduced by 1/2 while ours it will be by 1/4.</s><s coords="8,141.76,692.56,144.61,8.64;8,50.11,704.20,7.83,8.74;8,57.95,702.62,4.08,6.12;8,57.95,708.83,2.82,6.12;8,65.02,704.51,80.15,8.64">This is because we want to compute G * i over a larger region.</s></p><p><s coords="8,320.82,411.05,63.22,8.96;8,384.03,409.47,4.08,6.12;8,384.03,415.68,2.82,6.12;8,392.31,411.37,152.81,8.64;8,308.86,423.32,236.25,8.64;8,308.86,435.28,144.43,8.64">Even though G * i is usually computed over a larger region than in our framework, it still provides captures spatial convergence within a small region.</s><s coords="8,460.56,434.96,84.55,8.96;8,308.86,447.23,236.25,8.64;8,308.86,459.19,236.25,8.64">Also, two G-pooling operations are applied at different scales of feature map and so a larger region in the input image is really considered.</s><s coords="8,308.86,470.82,236.25,8.96;8,308.86,482.78,236.25,8.96;8,308.86,494.73,34.09,8.74">Specifically, the first 4 × 4 pooling window is slid over the 256 × 256 feature map and the output feature map has size 64 × 64.</s><s coords="8,346.02,495.05,199.09,8.64;8,308.86,506.69,104.33,8.96">This is fed through the next conv layers and a second G-pooling is applied.</s><s coords="8,419.46,507.01,125.65,8.64;8,308.86,518.64,236.25,8.96;8,308.86,530.60,236.25,8.96;8,308.86,542.87,79.13,8.64">At this stage, the input feature map is 64 × 64 and so when a 4 × 4 sliding window is now used, a region of 16 × 16 is really considered, which is 1/16 of the whole image.</s></p><p><s coords="8,308.86,573.98,218.87,9.03">Limitations There are some limitations of our work.</s><s coords="8,531.43,574.37,13.69,8.64;8,308.86,586.33,236.25,8.64;8,308.86,597.96,98.81,8.96;8,407.67,596.39,4.08,6.12;8,407.67,602.60,2.82,6.12;8,415.41,598.28,34.59,8.64">For example, we didn't investigate the optimal window size for performing Getis-Ord G * i analysis.</s><s coords="8,455.09,598.28,90.02,8.64;8,308.86,610.24,147.21,8.64">We also only consider one kind of spatial pattern, clusters.</s><s coords="8,462.17,610.24,82.95,8.64;8,308.86,622.19,236.25,8.64;8,308.86,634.15,53.39,8.64">And, there might be better places than pooling to incorporate knowledge in CNN architectures.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8." coords="8,308.86,659.01,69.09,10.75">Conclusion</head><p><s coords="8,320.82,680.60,224.30,8.64;8,308.86,692.56,236.25,8.64;8,308.86,704.51,51.10,8.64">In this paper, we investigate how geospatial knowledge can be incorporated into deep learning for geospatial image analysis.</s><s coords="8,363.63,704.51,181.49,8.64;9,50.11,75.48,114.99,8.64">We demonstrate that incorporating geospatial rules improves performance.</s><s coords="9,169.74,75.48,116.62,8.64;9,50.11,87.43,236.25,8.64;9,50.11,99.39,15.22,8.64">We realize, though, that ours is just preliminary work into geospatial guided deep learning.</s><s coords="9,72.71,99.39,213.66,8.64;9,50.11,111.34,236.25,8.64;9,50.11,123.30,215.68,8.64">We note the limitations of our approach, for example, that the prior distribution does not provide benefits for classes in which this prior knowledge is not relevant.</s><s coords="9,270.87,123.30,15.49,8.64;9,50.11,135.25,236.25,8.64;9,50.11,147.21,202.88,8.64">Our proposed approach does not show much improvement on the single dataset case especially a small dataset.</s><s coords="9,259.78,147.21,26.58,8.64;9,50.11,159.16,236.25,8.64;9,50.11,170.80,160.56,8.96">ISPRS Vaihingen is a very small dataset which contains around only 500 images of size of 256 × 256.</s><s coords="9,217.01,171.12,69.35,8.64;9,50.11,183.07,236.25,8.64;9,50.11,195.03,186.76,8.64">In the future, we will explore other ways to encode geographic rules so they can be incorporated into deep learning models.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,50.11,256.55,236.25,8.64;3,50.11,268.18,236.25,8.96;3,50.11,280.46,236.25,8.64;3,50.11,292.41,236.25,8.64;3,50.11,304.37,236.25,8.64;3,50.11,316.32,236.25,8.64;3,50.11,327.96,236.25,8.96;3,50.11,340.23,236.25,8.64;3,50.11,352.19,125.12,8.64;3,50.11,72.00,236.24,172.70"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s coords="3,50.11,256.55,236.25,8.64;3,50.11,268.18,236.25,8.96;3,50.11,280.46,236.25,8.64;3,50.11,292.41,131.49,8.64">Figure 2: Given a feature map as an input, max pooling (top right) and the proposed G-pooling (bottom right) create different output downsampled feature map based on the characteristics of spatial cluster.</s><s coords="3,188.90,292.41,97.46,8.64;3,50.11,304.37,236.25,8.64;3,50.11,316.32,12.45,8.64">The feature map within the sliding window (blue dot line) indicates a spatial cluster.</s><s coords="3,68.13,316.32,218.23,8.64;3,50.11,327.96,236.25,8.96;3,50.11,340.23,76.94,8.64">Max pooling takes the max value ignoring the spatial cluster, while our G-pooling takes the interpolated value at the center location.</s><s coords="3,131.42,340.23,154.95,8.64;3,50.11,352.19,125.12,8.64">(White, gray and black represent three values range from low to high.)</s></p></div></figDesc><graphic coords="3,50.11,72.00,236.24,172.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,59.03,184.06,218.42,8.96;4,56.02,72.00,224.43,100.54"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s coords="4,59.03,184.06,218.42,8.96">Figure 3: A FCN network architecture with G-pooling.</s></p></div></figDesc><graphic coords="4,56.02,72.00,224.43,100.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,50.11,365.94,495.00,8.64;8,50.11,377.89,67.00,8.64;8,50.11,72.00,495.03,282.09"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s coords="8,50.11,365.94,196.08,8.64">Figure 4: Qualitative results of ISPRS Potsdam.</s><s coords="8,251.51,365.94,293.61,8.64;8,50.11,377.89,67.00,8.64">White: road, blue: building, cyan: low vegetation, green: trees, yellow: cars, red: clutter.</s></p></div></figDesc><graphic coords="8,50.11,72.00,495.03,282.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,50.11,73.56,495.00,253.90"><head>Table 1 :</head><label>1</label><figDesc><div><p><s coords="5,86.82,73.88,233.55,8.64">Experimental results of FCN using VGG-16 as backbone.</s><s coords="5,324.96,73.56,220.15,8.96;5,50.11,85.84,173.12,8.64">Stride conv, P-pooling and ours G-pooling are used to replaced the standard max/average pooling.</s></p></div></figDesc><table coords="5,108.49,111.34,378.24,216.12"><row><cell>Potsdam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="5">Roads Buildings Low Veg. Trees Cars mIoU Pixel Acc.</cell></row><row><cell>Max</cell><cell>70.62</cell><cell>74.28</cell><cell>65.94</cell><cell>61.36 61.40 66.72</cell><cell>79.55</cell></row><row><cell>Average</cell><cell>69.34</cell><cell>74.49</cell><cell>63.94</cell><cell>60.06 60.28 65.62</cell><cell>78.08</cell></row><row><cell>Stride</cell><cell>67.22</cell><cell>73.97</cell><cell>63.01</cell><cell>60.09 59.39 64.74</cell><cell>77.54</cell></row><row><cell>P-pooling</cell><cell>71.97</cell><cell>75.55</cell><cell>66.80</cell><cell>62.03 62.39 67.75</cell><cell>81.02</cell></row><row><cell cols="2">G-pooling-1.0 (ours) 68.59</cell><cell>77.39</cell><cell>67.48</cell><cell>55.56 62.18 66.24</cell><cell>79.43</cell></row><row><cell cols="2">G-pooling-1.5 (ours) 70.06</cell><cell>76.12</cell><cell>67.67</cell><cell>62.12 63.91 67.98</cell><cell>81.63</cell></row><row><cell cols="2">G-pooling-2.0 (ours) 70.99</cell><cell>74.89</cell><cell>65.34</cell><cell>61.57 60.77 66.71</cell><cell>79.46</cell></row><row><cell>Vaihingen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max</cell><cell>70.63</cell><cell>80.42</cell><cell>51.57</cell><cell>70.12 55.32 65.61</cell><cell>81.88</cell></row><row><cell>Average</cell><cell>70.54</cell><cell>79.86</cell><cell>50.49</cell><cell>69.18 54.83 64.98</cell><cell>79.98</cell></row><row><cell>Strde conv</cell><cell>68.36</cell><cell>77.65</cell><cell>49.21</cell><cell>67.34 53.29 63.17</cell><cell>79.44</cell></row><row><cell>P-pooling</cell><cell>71.06</cell><cell>80.52</cell><cell>51.70</cell><cell>70.93 53.65 65.57</cell><cell>82.44</cell></row><row><cell cols="2">G-pooling-1.0 (ours) 72.15</cell><cell>79.69</cell><cell>53.28</cell><cell>70.89 53.72 65.95</cell><cell>81.78</cell></row><row><cell cols="2">G-pooling-1.5 (ours) 71.61</cell><cell>78.74</cell><cell>48.18</cell><cell>68.53 55.64 64.54</cell><cell>80.42</cell></row><row><cell cols="2">G-pooling-2.0 (ours) 71.09</cell><cell>78.88</cell><cell>50.62</cell><cell>68.32 54.01 64.58</cell><cell>80.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,50.11,73.56,495.00,199.26"><head>Table 2 :</head><label>2</label><figDesc><div><p><s coords="6,87.65,73.88,104.88,8.64">Cross-location evaluation.</s><s coords="6,197.95,73.56,347.16,8.96;6,50.11,85.84,217.61,8.64">We compare the generalization capability of using G-pooling with domain adaptation method AdaptSegNet which utilize the unlabeled data.</s></p></div></figDesc><table coords="6,115.27,111.02,364.68,161.80"><row><cell></cell><cell></cell><cell cols="3">Potsdam → Vaihingen</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">Roads Buildings Low Veg. Trees Cars mIoU Pixel Acc.</cell></row><row><cell>Max-pooling</cell><cell>28.75</cell><cell>51.10</cell><cell>13.48</cell><cell cols="3">56.00 25.99 35.06</cell><cell>47.48</cell></row><row><cell>stride conv</cell><cell>28.66</cell><cell>50.98</cell><cell>12.76</cell><cell cols="3">55.02 24.81 34.45</cell><cell>46.51</cell></row><row><cell>P-pooling</cell><cell>32.87</cell><cell>50.43</cell><cell>13.04</cell><cell cols="3">55.41 25.60 35.47</cell><cell>48.94</cell></row><row><cell cols="2">Ours (G-pooling) 37.27</cell><cell>54.53</cell><cell>14.85</cell><cell cols="3">54.24 27.35 37.65</cell><cell>55.20</cell></row><row><cell>AdaptSegNet</cell><cell>41.54</cell><cell>40.74</cell><cell>21.68</cell><cell cols="3">50.45 36.87 38.26</cell><cell>57.73</cell></row><row><cell></cell><cell></cell><cell cols="3">Vaihingen → Potsdam</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max-pooling</cell><cell>20.36</cell><cell>24.51</cell><cell>19.19</cell><cell>9.71</cell><cell>3.65</cell><cell>15.48</cell><cell>45.32</cell></row><row><cell>stride conv</cell><cell>20.65</cell><cell>23.22</cell><cell>16.57</cell><cell>8.73</cell><cell>8.32</cell><cell>15.50</cell><cell>42.28</cell></row><row><cell>P-pooling</cell><cell>23.97</cell><cell>27.66</cell><cell>14.03</cell><cell cols="3">10.30 12.07 19.61</cell><cell>44.98</cell></row><row><cell cols="2">Ours (G-pooling) 27.05</cell><cell>29.34</cell><cell>33.57</cell><cell cols="3">9.12 16.01 23.02</cell><cell>45.54</cell></row><row><cell>AdaptSegNet</cell><cell>40.28</cell><cell>37.97</cell><cell>46.11</cell><cell cols="3">15.87 20.16 32.08</cell><cell>50.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,50.11,432.15,236.25,75.01"><head>Table 3 :</head><label>3</label><figDesc><div><p><s coords="6,85.11,432.15,201.25,8.64;6,50.11,444.11,161.32,8.64">The average percentage of detected spatial clusters per feature map with different threshold.</s></p></div></figDesc><table coords="6,98.97,469.61,138.53,37.55"><row><cell>Threshold</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0</cell></row><row><cell>Potsdam</cell><cell cols="3">15.87 9.85 7.65</cell></row><row><cell cols="4">Vaihingen 14.99 10.44 7.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,308.86,162.59,236.25,240.57"><head>Table 4 :</head><label>4</label><figDesc><div><p><s coords="7,343.27,162.59,201.84,8.64;7,308.86,174.23,236.25,8.96;7,308.86,186.50,26.74,8.64">Experimental results on comparing w/o and w/ proposed G-pooling for the state-of-the-art segmentation networks.</s><s coords="7,340.93,186.50,204.18,8.64;7,308.86,198.46,156.30,8.64">P→V indicates the model trained on Potsdam and test on Vaihingen, and versa the verses.</s></p></div></figDesc><table coords="7,314.84,223.96,229.63,179.20"><row><cell></cell><cell>Potsdam (P)</cell><cell></cell><cell>P→V</cell></row><row><cell cols="2">Network G-Pooling mIoU</cell><cell>PA</cell><cell>mIoU</cell><cell>PA</cell></row><row><cell>×</cell><cell cols="4">67.97 81.25 38.57 58.47</cell></row><row><cell>DeepLab</cell><cell cols="4">68.33 80.67 40.04 63.21</cell></row><row><cell>×</cell><cell cols="4">69.47 82.53 35.98 53.69</cell></row><row><cell>SegNet</cell><cell cols="4">70.17 83.27 39.04 56.42</cell></row><row><cell>×</cell><cell cols="4">66.72 79.55 35.06 47.48</cell></row><row><cell>FCN</cell><cell cols="4">67.98 81.63 37.65 55.20</cell></row><row><cell></cell><cell>Vaihingen (V)</cell><cell></cell><cell>V→P</cell></row><row><cell>×</cell><cell cols="4">70.80 83.74 18.44 33.96</cell></row><row><cell>DeepLab</cell><cell cols="4">70.11 83.09 19.26 36.17</cell></row><row><cell>×</cell><cell cols="4">66.04 81.79 16.77 45.90</cell></row><row><cell>SegNet</cell><cell cols="4">66.71 82.66 25.64 48.08</cell></row><row><cell>×</cell><cell cols="4">65.61 81.88 15.48 45.32</cell></row><row><cell>FCN</cell><cell cols="4">65.95 81.87 23.02 45.54</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,70.04,239.91,169.08,7.77;9,259.46,240.75,26.90,6.31;9,70.03,251.71,209.81,6.31;9,70.03,261.83,128.81,7.77" xml:id="b0">
	<monogr>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html.4" />
		<title level="m">ISPRS 2D Semantic Labeling Challenge</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,274.38,216.32,7.77;9,70.03,285.34,216.33,7.77;9,70.03,296.14,216.33,7.93;9,70.03,307.10,79.93,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very High Resolution Urban Remote Sensing with Multimodal Deep Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lefvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,319.81,216.32,7.77;9,70.03,330.77,216.33,7.77;9,70.03,341.57,216.34,7.93;9,70.03,352.53,175.52,7.93" xml:id="b2">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007">2017. 2, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,365.24,216.32,7.77;9,70.03,376.20,216.33,7.77;9,70.03,387.00,216.33,7.93;9,70.03,397.95,109.57,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main">Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools, and Challenges for the Community</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ap</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>plied Remote Sensing</note>
</biblStruct>

<biblStruct coords="9,70.04,410.67,216.32,7.77;9,70.03,421.63,216.33,7.77;9,70.03,432.59,216.33,7.77;9,70.03,443.38,160.87,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-guided Golf Course Detection using a Convolutional Neural Network Fine-tuned on Temporally Augmented Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,456.10,216.32,7.77;9,70.03,467.06,216.33,7.77;9,70.03,478.02,216.33,7.77;9,70.03,488.81,216.33,7.93;9,70.03,499.77,159.82,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,512.49,216.32,7.77;9,70.03,523.45,216.33,7.77;9,70.03,534.24,160.30,7.93" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<title level="m">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,70.04,546.96,216.32,7.77;9,70.03,557.92,216.33,7.77;9,70.03,568.71,216.33,7.93;9,70.03,579.67,129.99,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,592.39,216.32,7.77;9,70.03,603.34,216.33,7.77;9,70.03,614.14,216.33,7.93;9,70.03,625.26,13.45,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main">Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,637.82,216.32,7.77;9,70.03,648.77,216.33,7.77;9,70.03,659.57,216.33,7.93;9,70.03,670.53,104.43,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main">No More Discrimination: Cross City Adaptation of Road Scene Segmenters</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,70.04,683.24,216.32,7.77;9,70.03,694.04,216.33,7.93;9,70.03,705.00,206.77,7.93;9,308.86,76.13,236.25,7.77;9,328.78,87.08,216.33,7.77;9,328.78,97.88,216.33,7.94;9,328.78,108.84,109.94,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-Consistent Adversarial Domain Adaptation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2018</date>
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct coords="9,328.79,121.84,216.32,7.77;9,328.78,132.80,216.33,7.77;9,328.78,143.60,170.75,7.93" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">FCNs in the Wild: Pixel-Level Adversarial and Constraint-based Adaptation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,328.79,156.59,216.32,7.77;9,328.78,167.55,216.33,7.77;9,328.78,178.35,216.33,7.73;9,328.78,189.31,83.92,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,202.31,216.32,7.77;9,328.78,213.27,216.33,7.77;9,328.78,224.07,216.33,7.93;9,328.78,235.18,27.89,7.77" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11431</idno>
		<title level="m">Physics-Guided Neural Networks (PGNN): An Application in Lake Temperature Modeling</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,328.79,248.02,216.32,7.77;9,328.78,258.98,216.33,7.77;9,328.78,269.78,216.33,7.93;9,328.78,280.74,75.95,7.93" xml:id="b14">
	<analytic>
		<title level="a" type="main">DelugeNets: Deep Networks with Efficient and Flexible Cross-Layer Information Inflows</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,293.74,216.32,7.77;9,328.78,304.70,216.33,7.77;9,328.78,315.49,216.33,7.93;9,328.78,326.45,96.29,7.93" xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data. IEEE Geoscience and Remote Sensing Letters</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lavreniuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shelestov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,339.45,216.32,7.77;9,328.78,350.41,216.33,7.77;9,328.78,361.21,186.18,7.93" xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,374.21,216.32,7.77;9,328.78,385.16,216.33,7.77;9,328.78,395.96,216.33,7.93;9,328.78,406.92,167.87,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,419.92,216.32,7.77;9,328.78,430.72,216.33,7.93;9,328.78,441.68,216.33,7.93;9,328.78,452.80,22.42,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,465.63,216.32,7.77;9,328.78,476.59,216.33,7.77;9,328.78,487.55,216.33,7.77;9,328.78,498.35,60.17,7.93" xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Traffic as Images: A Deep Convolutional Neural Network for Large-Scale Transportation Network Speed Prediction</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,511.35,216.32,7.77;9,328.78,522.31,216.33,7.77;9,328.78,533.10,216.33,7.93;9,328.78,544.06,60.26,7.93" xml:id="b20">
	<analytic>
		<title level="a" type="main">High-Resolution Aerial Image Labeling with Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,557.06,216.32,7.77;9,328.78,567.86,216.33,7.93;9,328.78,578.82,106.92,7.93" xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Detect Roads in High-Resolution Aerial Images</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,591.82,216.32,7.77;9,328.78,602.78,216.33,7.77;9,328.78,613.57,216.33,7.93;9,328.78,624.53,134.66,7.93" xml:id="b22">
	<analytic>
		<title level="a" type="main">A Relation-Augmented Fully Convolutional Network for Semantic Segmentation in Aerial Scenes</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,637.53,216.32,7.77;9,328.78,648.33,216.33,7.93;9,328.78,659.29,99.73,7.93" xml:id="b23">
	<monogr>
		<title level="m" type="main">Local Spatial Autocorrelation Statistics: Distributional Issues and an Application. Geographical Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Ord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Getis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,328.79,672.29,216.32,7.77;9,328.78,683.24,216.33,7.77;9,328.78,694.04,216.33,7.73;9,328.78,705.00,151.98,7.93" xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,76.13,216.32,7.77;10,70.03,86.92,216.33,7.94;10,70.03,97.88,216.33,7.94;10,70.03,109.00,13.45,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main">Detailpreserving pooling in deep networks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Saeedan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,120.96,216.32,7.77;10,70.03,131.75,216.33,7.93;10,70.03,142.71,129.17,7.93" xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</title>
		<author>
			<persName coords=""><forename type="first">Jamie</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,154.83,216.32,7.77;10,70.03,165.63,216.33,7.93;10,70.03,176.59,102.35,7.93" xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,188.70,216.32,7.77;10,70.03,199.50,216.33,7.93;10,70.03,210.46,216.33,7.73;10,70.03,221.42,94.88,7.93" xml:id="b28">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation workshop (ICLR workshop)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,233.53,216.32,7.77;10,70.03,244.33,191.88,7.93" xml:id="b29">
	<analytic>
		<title level="a" type="main">A Computer Movie Simulating Urban Growth in the Detroit Region</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Geography</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,256.45,216.32,7.77;10,70.03,267.41,216.33,7.77;10,70.03,278.21,216.33,7.93;10,70.03,289.16,192.79,7.93" xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to Adapt Structured Output Space for Semantic Segmentation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,301.28,216.32,7.77;10,70.03,312.24,216.33,7.77;10,70.03,323.04,216.33,7.93;10,70.03,334.00,58.77,7.93" xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Structured Output via Discriminative Representations</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,346.11,216.32,7.77;10,70.03,356.91,216.33,7.93;10,70.03,367.87,212.96,7.93" xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,379.99,216.32,7.77;10,70.03,390.94,216.33,7.77;10,70.03,401.74,216.33,7.93;10,70.03,412.70,216.33,7.73;10,70.03,423.66,69.72,7.93" xml:id="b33">
	<analytic>
		<title level="a" type="main">Building detail-sensitive semantic segmentation networks with polynomial pooling</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,435.78,216.32,7.77;10,70.03,446.57,216.33,7.93;10,70.03,457.53,216.33,7.93;10,70.03,468.65,4.48,7.77" xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Building Extraction in Aerial Scenes with Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,480.61,216.32,7.77;10,70.03,491.41,151.33,7.93" xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Wide Residual Networks. arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,70.04,503.52,216.32,7.77;10,70.03,514.48,216.33,7.77;10,70.03,525.28,216.33,7.93;10,70.03,536.24,137.59,7.93" xml:id="b36">
	<analytic>
		<title level="a" type="main">Airport Detection from Remote Sensing Images using Transferable Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,548.35,216.32,7.77;10,70.03,559.15,216.33,7.93;10,70.03,570.11,159.32,7.93" xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,582.23,216.32,7.77;10,70.03,593.19,216.33,7.77;10,70.03,603.98,216.33,7.93;10,70.03,614.94,142.60,7.93" xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.04,627.06,216.32,7.77;10,70.03,638.02,216.33,7.77;10,70.03,648.82,216.33,7.93;10,70.03,659.77,216.33,7.73;10,70.03,670.89,27.89,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
