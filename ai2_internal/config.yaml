config_version: 0.0.1
model_variants:
  ivila-row-layoutlm-finetuned-s2vl-v2:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.vila.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.vila.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.vila.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.vila.interface.PredictorConfig

    integration_test: ai2_internal.vila.integration_test.TestInterfaceIntegration

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "vila_predictors"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    # https://huggingface.co/allenai/ivila-row-layoutlm-finetuned-s2vl-v2
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/vila/ivila-row-layoutlm-finetuned-s2vl-v2.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    cuda: True

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: ["apt-get update", "apt-get install python3-opencv -y"]

  layout_parser:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.layout_parser.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.layout_parser.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.layout_parser.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.layout_parser.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "lp_predictors"]

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    # Whether this model supports CUDA GPU acceleration
    cuda: True

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.layout_parser.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: ["apt-get update", "apt-get install python3-opencv -y"]

  bibentry_predictor:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.bibentry_predictor.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.bibentry_predictor.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.bibentry_predictor.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.bibentry_predictor.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "bibentry_predictor"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/bibentryparser/v0.onnx.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    # Whether this model supports CUDA GPU acceleration
    cuda: False

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.bibentry_predictor.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: []

  bibentry_predictor_mmda:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.bibentry_predictor_mmda.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.bibentry_predictor_mmda.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.bibentry_predictor_mmda.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.bibentry_predictor_mmda.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "bibentry_predictor_gpu"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/bibentryparser/v0.onnx.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    # Whether this model supports CUDA GPU acceleration
    cuda: True

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.bibentry_predictor_mmda.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: []

  citation_mentions:
    # Class path to pydantic Instance implementation
    instance: ai2_internal.citation_mentions.interface.Instance

    # Class path to pydantic Prediction implementation
    prediction: ai2_internal.citation_mentions.interface.Prediction

    # Class path to Predictor implementation
    predictor: ai2_internal.citation_mentions.interface.Predictor

    # Class path to pydantic PredictorConfig implementation
    predictor_config: ai2_internal.citation_mentions.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    extras_require: ["dev", "mention_predictor_gpu"]

    # Full S3 path to tar.gz'ed artifacts archive
    # python -m transformers.onnx -m . --feature token-classification --framework pt --preprocessor tokenizer --opset 16 .
    # ref: https://github.com/pytorch/pytorch/blob/v1.12.0/torch/onnx/__init__.py#L215-L217
    artifacts_s3_path: s3://ai2-s2-mmda/models/citation-mentions/2022-07-27-minilm-10k/model/artifacts-onnx16.tar.gz

    # Version of python required for model runtime
    python_version: "3.8"

    # Whether this model supports CUDA GPU acceleration
    cuda: True

    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.citation_mentions.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    docker_run_commands: ["apt-get update && apt-get install -y poppler-utils"]

  citation_links:
    # Class path to pydantic Instance implementation
    instance: ai2_internal.citation_links.interface.Instance

    # Class path to pydantic Prediction implementation
    prediction: ai2_internal.citation_links.interface.Prediction

    # Class path to Predictor implementation
    predictor: ai2_internal.citation_links.interface.Predictor

    # Class path to pydantic PredictorConfig implementation
    predictor_config: ai2_internal.citation_links.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    extras_require: ["dev", "citation_links"]

    # Full S3 path to tar.gz'ed artifacts archive
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/citationlinks/v0.tar.gz

    # Version of python required for model runtime
    python_version: "3.8"

    # Whether this model supports CUDA GPU acceleration
    cuda: False

    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.citation_links.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    docker_run_commands: []

  bibentry_detection_predictor:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.bibentry_detection_predictor.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.bibentry_detection_predictor.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.bibentry_detection_predictor.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.bibentry_detection_predictor.interface.PredictorConfig

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    artifacts_s3_path: s3://ai2-s2-mmda/models/bibliography-entries/outputs/p3_2xl/bibs/silver_dataset_8k/one_category/pln_mask_rcnn_X_101_32x8d_FPN_3x/archive.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: "3.8"

    # Whether this model supports CUDA GPU acceleration
    cuda: true

    # One of the versions here: https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md#ubuntu2004, but less than 11.4.3.
    # If cuda=True and cuda_version is unspecified, defaults to 11.4.2.
    cuda_version: "11.1.1"

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.bibentry_detection_predictor.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile AFTER extras require.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.

    # pip installing dependencies listed in setup.py so that detectron2 installs successfully
    docker_run_commands: [ "apt-get update && apt-get install -y poppler-utils libgl1",
                           "pip install layoutparser",
                           "pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html",
                           "pip install 'detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2'"]

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: [ "bibentry_detection_predictor" ]

  figure_table_predictors:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.figure_table_predictors.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.figure_table_predictors.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.figure_table_predictors.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.figure_table_predictors.interface.PredictorConfig

    integration_test: ai2_internal.figure_table_predictors.integration_test.TestInterfaceIntegration

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "vila_predictors"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    # https://huggingface.co/allenai/ivila-row-layoutlm-finetuned-s2vl-v2
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/vila/ivila-row-layoutlm-finetuned-s2vl-v2.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    cuda: True

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: ["apt-get update", "apt-get install python3-opencv -y"]
