config_version: 0.0.1
model_variants:
  ivila-row-layoutlm-finetuned-s2vl-v2:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.vila.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.vila.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.vila.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.vila.interface.PredictorConfig

    integration_test: ai2_internal.vila.integration_test.TestInterfaceIntegration

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "vila_predictors"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    # https://huggingface.co/allenai/ivila-row-layoutlm-finetuned-s2vl-v2
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/vila/ivila-row-layoutlm-finetuned-s2vl-v2.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    cuda: True

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: ["apt-get update", "apt-get install python3-opencv -y"]

  layout_parser:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.layout_parser.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.layout_parser.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.layout_parser.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.layout_parser.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "lp_predictors"]

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.8

    # Whether this model supports CUDA GPU acceleration
    cuda: False

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.layout_parser.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: ["apt-get update", "apt-get install python3-opencv -y"]

  bibentry_predictor:
    # Class path to pydantic Instance implementation in <model_package_name>==<model_package_version>
    instance: ai2_internal.bibentry_predictor.interface.Instance

    # Class path to pydantic Prediction implementation in <model_package_name>==<model_package_version>
    prediction: ai2_internal.bibentry_predictor.interface.Prediction

    # Class path to Predictor implementation in <model_package_name>==<model_package_version>
    predictor: ai2_internal.bibentry_predictor.interface.Predictor

    # Class path to pydantic PredictorConfig implementation in <model_package_name>==<model_package_version>
    predictor_config: ai2_internal.bibentry_predictor.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    # These are the 'extras_require' keys in your setup.py.
    extras_require: ["dev", "bibentry_predictor"]

    # Full S3 path to tar.gz'ed artifacts archive, nullable
    artifacts_s3_path: s3://ai2-timo-registry/model-artifacts/bibentryparser/v0.tar.gz

    # Version of python required for model runtime, e.g. 3.7, 3.8, 3.9
    python_version: 3.7

    # Whether this model supports CUDA GPU acceleration
    cuda: False

    # Python path to a fn in <model_package_name>==<model_package_version> that
    # returns a unittest.TestCase. Builder function receives a model container
    # as its sole argument.
    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.bibentry_predictor.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    # Leave this unset unless your model has special system requirements beyond
    # those in your setup.py.
    docker_run_commands: []

  citation_mentions: &citation_mentions_base_config
    # Class path to pydantic Instance implementation
    instance: ai2_internal.citation_mentions.interface.Instance

    # Class path to pydantic Prediction implementation
    prediction: ai2_internal.citation_mentions.interface.Prediction

    # Class path to Predictor implementation
    predictor: ai2_internal.citation_mentions.interface.Predictor

    # Class path to pydantic PredictorConfig implementation
    predictor_config: ai2_internal.citation_mentions.interface.PredictorConfig

    # Any additional sets of dependencies required by the model.
    extras_require: ["dev", "mention_predictor"]

    # Full S3 path to tar.gz'ed artifacts archive
    artifacts_s3_path: s3://ai2-s2-mmda/models/citation-mentions/2022-07-27-minilm-10k/model/artifacts.tar.gz

    # Version of python required for model runtime
    python_version: "3.8"

    # Whether this model supports CUDA GPU acceleration
    cuda: False

    # Used by the TIMO toolchain to validate your model implementation and configuration.
    integration_test: ai2_internal.citation_mentions.integration_test.TestInterfaceIntegration

    # One or more bash commands to execute as part of a RUN step in a Dockerfile.
    docker_run_commands: ["apt-get update && apt-get install -y poppler-utils"]

  citation_mentions__onnx:
    << : *citation_mentions_base_config

    # python -m transformers.onnx -m . --feature token-classification --framework pt --preprocessor tokenizer --opset 16 .
    # ref: https://github.com/pytorch/pytorch/blob/v1.12.0/torch/onnx/__init__.py#L215-L217
    artifacts_s3_path: s3://ai2-s2-mmda/models/citation-mentions/2022-07-27-minilm-10k/model/artifacts-onnx16.tar.gz
    cuda: True
